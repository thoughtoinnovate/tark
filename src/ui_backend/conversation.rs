//! Conversation Service - Manages conversation lifecycle
//!
//! Orchestrates ConversationManager and ChatAgent to handle the full chat flow
//! including streaming and context management.

use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::Arc;
use tokio::sync::mpsc;

use crate::agent::ChatAgent;
use crate::core::context_tracker::ContextBreakdown;
use crate::core::conversation_manager::ConversationManager;
use crate::core::types::AgentMode;
use crate::llm::LlmProvider;
use crate::storage::ChatSession;
use crate::tools::InteractionSender;
use crate::tools::ToolRegistry;

use super::errors::ConversationError;
use super::events::AppEvent;

/// Context usage information
#[derive(Debug, Clone)]
pub struct ContextUsage {
    pub used_tokens: usize,
    pub max_tokens: usize,
    pub percent: f32,
}

/// Result of context compaction
#[derive(Debug, Clone)]
pub struct CompactionResult {
    pub old_tokens: usize,
    pub new_tokens: usize,
    pub messages_removed: usize,
    /// Summary generated by compaction (if LLM summarization was used)
    pub summary: Option<String>,
}

/// Conversation Service
///
/// Orchestrates the chat flow by coordinating:
/// - ConversationManager (message history + streaming)
/// - ChatAgent (LLM communication + context tracking)
pub struct ConversationService {
    /// Conversation manager for message history and streaming
    conversation_mgr: Arc<tokio::sync::RwLock<ConversationManager>>,
    /// Chat agent for LLM communication and context tracking
    chat_agent: Arc<tokio::sync::RwLock<ChatAgent>>,
    /// Event channel for streaming updates
    event_tx: mpsc::UnboundedSender<AppEvent>,
    /// Interaction channel (ask_user / approval)
    interaction_tx: Option<InteractionSender>,
    /// Interrupt flag
    interrupt_flag: Arc<AtomicBool>,
    /// Processing flag
    is_processing: Arc<AtomicBool>,
}

impl ConversationService {
    /// Create a new conversation service
    pub fn new(chat_agent: ChatAgent, event_tx: mpsc::UnboundedSender<AppEvent>) -> Self {
        let conversation_mgr = Arc::new(tokio::sync::RwLock::new(ConversationManager::new()));
        let chat_agent = Arc::new(tokio::sync::RwLock::new(chat_agent));

        Self {
            conversation_mgr,
            chat_agent,
            event_tx,
            interaction_tx: None,
            interrupt_flag: Arc::new(AtomicBool::new(false)),
            is_processing: Arc::new(AtomicBool::new(false)),
        }
    }

    /// Create a new conversation service with interaction support
    pub fn new_with_interaction(
        chat_agent: ChatAgent,
        event_tx: mpsc::UnboundedSender<AppEvent>,
        interaction_tx: Option<InteractionSender>,
    ) -> Self {
        let service = Self::new(chat_agent, event_tx);
        Self {
            interaction_tx,
            ..service
        }
    }

    /// Update the active LLM provider for the chat agent
    pub async fn update_llm_provider(
        &self,
        llm: Arc<dyn LlmProvider>,
    ) -> Result<(), ConversationError> {
        self.update_llm_provider_with_context(llm, None).await
    }

    /// Update the LLM provider with an optional context limit from models.dev
    ///
    /// When `context_limit` is provided, it will be used to set the ChatAgent's
    /// max context tokens. This should be called with the model's context limit
    /// from models.dev when switching models.
    pub async fn update_llm_provider_with_context(
        &self,
        llm: Arc<dyn LlmProvider>,
        context_limit: Option<usize>,
    ) -> Result<(), ConversationError> {
        if self.is_processing.load(Ordering::SeqCst) {
            return Err(ConversationError::Other(anyhow::anyhow!(
                "Cannot switch provider while a message is streaming"
            )));
        }

        let mut agent = self.chat_agent.write().await;

        // Check current context usage before switching
        let old_max = agent.max_context_tokens();
        let current_tokens = agent.estimated_context_tokens();

        // Update the provider
        agent.update_provider(llm);
        agent.refresh_system_prompt_async().await;

        // Update context limit from models.dev if provided
        if let Some(limit) = context_limit {
            if limit > 0 {
                tracing::info!(
                    "Updating context limit from models.dev: {} -> {} tokens",
                    old_max,
                    limit
                );
                agent.set_max_context_tokens(limit);
            }
        }

        // Check if new model has smaller context window
        let new_max = agent.max_context_tokens();
        if new_max < old_max {
            let threshold = (new_max as f32 * 0.80) as usize;
            if current_tokens > threshold {
                tracing::warn!(
                    "Model switch: current context ({} tokens) exceeds 80% of new limit ({} tokens). \
                     Context will be auto-compacted on next message.",
                    current_tokens,
                    new_max
                );
                // The auto-compaction check in send_message() will handle this
                // on the next prompt since should_compact() checks against current max
            }
        }

        Ok(())
    }

    /// Update tool approval trust level for the chat agent
    pub async fn set_trust_level(
        &self,
        level: crate::tools::TrustLevel,
    ) -> Result<(), ConversationError> {
        let mut agent = self.chat_agent.write().await;
        agent.set_trust_level(level).await;
        Ok(())
    }

    /// Send a message and stream the response
    pub async fn send_message(
        &self,
        content: &str,
        correlation_id: Option<String>,
    ) -> Result<(), ConversationError> {
        // Check if already processing
        if self.is_processing.load(Ordering::SeqCst) {
            return Err(ConversationError::Other(anyhow::anyhow!(
                "Already processing a message"
            )));
        }

        self.is_processing.store(true, Ordering::SeqCst);
        self.interrupt_flag.store(false, Ordering::SeqCst);

        // Check if auto-compaction is needed (80% threshold)
        if self.should_compact().await {
            tracing::info!("Context at 80%+ capacity, triggering auto-compaction");
            match self.compact_context().await {
                Ok(result) => {
                    tracing::info!(
                        "Auto-compacted: {} -> {} tokens ({} messages removed)",
                        result.old_tokens,
                        result.new_tokens,
                        result.messages_removed
                    );
                    // Emit proper compaction event
                    let _ = self.event_tx.send(AppEvent::ContextCompacted {
                        old_tokens: result.old_tokens,
                        new_tokens: result.new_tokens,
                        messages_removed: result.messages_removed,
                        summary: result.summary,
                    });
                }
                Err(e) => {
                    tracing::warn!("Auto-compaction failed: {}", e);
                    // Continue anyway - the LLM might still work
                }
            }
        }

        // Get correlation_id (use provided or generate new)
        let correlation_id = correlation_id.unwrap_or_else(|| uuid::Uuid::new_v4().to_string());

        // Log LLM request start (fast-path check avoids overhead when disabled)
        if crate::is_debug_logging_enabled() {
            if let Some(logger) = crate::debug_logger() {
                let entry: crate::DebugLogEntry = crate::DebugLogEntry::new(
                    correlation_id.clone(),
                    crate::LogCategory::Service,
                    "llm_request_start",
                )
                .with_data(serde_json::json!({
                    "content_length": content.len()
                }));
                logger.log(entry);
            }
        }

        // Add user message
        {
            let mut conv = self.conversation_mgr.write().await;
            conv.add_user_message(content.to_string());
        }

        // Start streaming
        {
            let mut conv = self.conversation_mgr.write().await;
            conv.start_streaming().map_err(|e| {
                ConversationError::Other(anyhow::anyhow!("Failed to start streaming: {}", e))
            })?;
        }

        // Emit started event
        let _ = self.event_tx.send(AppEvent::LlmStarted);

        // Create streaming callbacks
        let event_tx_text = self.event_tx.clone();
        let event_tx_thinking = self.event_tx.clone();
        let event_tx_tool_start = self.event_tx.clone();
        let event_tx_tool_complete = self.event_tx.clone();
        let interrupt_flag = self.interrupt_flag.clone();

        let check_interrupt = move || interrupt_flag.load(Ordering::SeqCst);

        let on_text = move |chunk: String| {
            let _ = event_tx_text.send(AppEvent::LlmTextChunk(chunk));
        };

        let on_thinking = move |chunk: String| {
            let _ = event_tx_thinking.send(AppEvent::LlmThinkingChunk(chunk));
        };

        let event_tx_commit = self.event_tx.clone();
        let on_commit_intermediate = move |content: String| {
            if !content.is_empty() {
                let _ = event_tx_commit.send(AppEvent::CommitIntermediateResponse(content));
            }
        };

        let correlation_id_tool_start = correlation_id.clone();
        let on_tool_call = move |tool_name: String, tool_args: String| {
            if let Ok(args_json) = serde_json::from_str::<serde_json::Value>(&tool_args) {
                // Log tool invocation (fast-path check for zero-cost when disabled)
                if crate::is_debug_logging_enabled() {
                    if let Some(logger) = crate::debug_logger() {
                        let entry: crate::DebugLogEntry = crate::DebugLogEntry::new(
                            correlation_id_tool_start.clone(),
                            crate::LogCategory::Service,
                            "tool_invocation",
                        )
                        .with_data(serde_json::json!({
                            "tool": tool_name.clone(),
                            "args": args_json.clone()
                        }));
                        logger.log(entry);
                    }
                }

                let _ = event_tx_tool_start.send(AppEvent::ToolStarted {
                    name: tool_name,
                    args: args_json,
                });
            }
        };

        let correlation_id_tool_complete = correlation_id.clone();
        let on_tool_complete = move |tool_name: String, result: String, success: bool| {
            // Log tool response (fast-path check for zero-cost when disabled)
            if crate::is_debug_logging_enabled() {
                let result_preview = if result.len() > 500 {
                    format!(
                        "{}...",
                        crate::core::truncate_at_char_boundary(&result, 500)
                    )
                } else {
                    result.clone()
                };

                if let Some(logger) = crate::debug_logger() {
                    let entry: crate::DebugLogEntry = crate::DebugLogEntry::new(
                        correlation_id_tool_complete.clone(),
                        crate::LogCategory::Service,
                        "tool_response",
                    )
                    .with_data(serde_json::json!({
                        "tool": tool_name.clone(),
                        "success": success,
                        "result_preview": result_preview,
                        "result_length": result.len()
                    }));
                    logger.log(entry);
                }
            }

            if success {
                let _ = event_tx_tool_complete.send(AppEvent::ToolCompleted {
                    name: tool_name,
                    result,
                });
            } else {
                let _ = event_tx_tool_complete.send(AppEvent::ToolFailed {
                    name: tool_name,
                    error: result,
                });
            }
        };

        // Send to LLM with streaming
        let result = {
            let mut agent = self.chat_agent.write().await;
            agent
                .chat_streaming(
                    content,
                    check_interrupt,
                    on_text,
                    on_thinking,
                    on_tool_call,
                    on_tool_complete,
                    on_commit_intermediate,
                )
                .await
        };

        // Finalize streaming
        {
            let mut conv = self.conversation_mgr.write().await;
            match result {
                Ok(response) => {
                    // Log LLM response (fast-path check for zero-cost when disabled)
                    if crate::is_debug_logging_enabled() {
                        if let Some(logger) = crate::debug_logger() {
                            logger.log(
                                crate::DebugLogEntry::new(
                                    correlation_id.clone(),
                                    crate::LogCategory::Service,
                                    "llm_response",
                                )
                                .with_data(serde_json::json!({
                                    "text_length": response.text.len(),
                                    "tool_calls_made": response.tool_calls_made,
                                    "input_tokens": response.usage.as_ref().map(|u| u.input_tokens),
                                    "output_tokens": response.usage.as_ref().map(|u| u.output_tokens),
                                })),
                            );
                        }
                    }

                    // Add assistant message
                    conv.add_assistant_message(response.text.clone(), None);
                    conv.clear_streaming();

                    // Emit completion event
                    let (input_tokens, output_tokens) = response
                        .usage
                        .map(|u| (u.input_tokens as usize, u.output_tokens as usize))
                        .unwrap_or((0, 0));

                    let _ = self.event_tx.send(AppEvent::LlmCompleted {
                        text: response.text,
                        thinking: response.thinking,
                        input_tokens,
                        output_tokens,
                    });
                }
                Err(e) => {
                    // Log LLM error with full context (fast-path check for zero-cost when disabled)
                    if crate::is_debug_logging_enabled() {
                        let error_context =
                            crate::debug_logger::ErrorContext::from_error(e.as_ref());
                        if let Some(logger) = crate::debug_logger() {
                            let entry1: crate::DebugLogEntry = crate::DebugLogEntry::new(
                                correlation_id.clone(),
                                crate::LogCategory::Service,
                                "llm_error",
                            )
                            .with_data(serde_json::json!({
                                "error_message": e.to_string()
                            }));
                            logger.log(entry1);

                            let entry2: crate::DebugLogEntry = crate::DebugLogEntry::new(
                                correlation_id.clone(),
                                crate::LogCategory::Service,
                                "llm_error_detail",
                            )
                            .with_error_context(error_context);
                            logger.log(entry2);
                        }
                    }

                    conv.handle_error(e.to_string(), false);
                    conv.clear_streaming();
                    let _ = self.event_tx.send(AppEvent::LlmError(e.to_string()));
                }
            }
        }

        self.is_processing.store(false, Ordering::SeqCst);
        Ok(())
    }

    /// Update agent mode and tool registry
    pub async fn update_mode(
        &self,
        working_dir: std::path::PathBuf,
        mode: AgentMode,
        trust_level: crate::tools::TrustLevel,
        tool_timeout_secs: u64,
        todo_tracker: Option<Arc<std::sync::Mutex<crate::tools::TodoTracker>>>,
        thinking_tracker: Option<Arc<std::sync::Mutex<crate::tools::ThinkingTracker>>>,
    ) {
        let mut tools = ToolRegistry::for_mode_with_services(
            working_dir,
            mode,
            true,
            self.interaction_tx.clone(),
            None,
            todo_tracker,
            thinking_tracker,
        );
        tools.set_tool_timeout_secs(tool_timeout_secs);
        // CRITICAL: Preserve trust level across mode changes to prevent security bypass
        tools.set_trust_level(trust_level);

        let mut agent = self.chat_agent.write().await;
        agent.update_mode(tools, mode);
    }

    /// Refresh the agent's system prompt
    /// Used when thinking_tool_enabled changes to inject/remove tool instructions
    pub async fn refresh_system_prompt(&self) {
        let mut agent = self.chat_agent.write().await;
        agent.refresh_system_prompt_async().await;
    }

    /// Set thinking tool enabled and refresh system prompt
    pub async fn set_thinking_tool_enabled(&self, enabled: bool) {
        let mut agent = self.chat_agent.write().await;
        agent.set_thinking_tool_enabled(enabled);
        agent.refresh_system_prompt_async().await;
    }

    /// Interrupt the current operation
    pub fn interrupt(&self) {
        self.interrupt_flag.store(true, Ordering::SeqCst);
        // Also reset processing flag to allow new messages after interruption
        // The streaming loop will see the interrupt flag and exit early
        self.is_processing.store(false, Ordering::SeqCst);
    }

    /// Interrupt and reset all streaming state (async version)
    /// This should be called when user explicitly cancels to ensure clean state
    pub async fn interrupt_and_reset(&self) {
        // Set interrupt flags
        self.interrupt_flag.store(true, Ordering::SeqCst);
        self.is_processing.store(false, Ordering::SeqCst);

        // Also clear the streaming state in conversation manager
        // This is crucial to prevent "Invalid streaming state transition" errors
        let mut conv = self.conversation_mgr.write().await;
        conv.clear_streaming();
    }

    /// Check if currently processing
    pub fn is_processing(&self) -> bool {
        self.is_processing.load(Ordering::SeqCst)
    }

    /// Get current context usage
    ///
    /// Uses non-blocking try_read to avoid UI freezes during LLM streaming.
    /// Returns default values if the lock is contended.
    pub async fn context_usage(&self) -> ContextUsage {
        // Try to get the lock without blocking - during streaming, the write lock
        // is held by chat_streaming, and we don't want to freeze the UI waiting
        match self.chat_agent.try_read() {
            Ok(agent) => {
                let used_tokens = agent.estimated_context_tokens();
                let max_tokens = agent.max_context_tokens();
                let percent = if max_tokens == 0 {
                    0.0
                } else {
                    (used_tokens as f32 / max_tokens as f32) * 100.0
                };

                ContextUsage {
                    used_tokens,
                    max_tokens,
                    percent,
                }
            }
            Err(_) => {
                // Lock is contended (streaming in progress) - return placeholder
                // The UI will update properly once streaming completes
                ContextUsage {
                    used_tokens: 0,
                    max_tokens: 100_000, // Safe default
                    percent: 0.0,
                }
            }
        }
    }

    /// Get detailed context breakdown by source
    ///
    /// Returns token usage broken down into:
    /// - system_prompt: tokens in the system prompt
    /// - conversation_history: tokens in user/assistant messages
    /// - tool_schemas: estimated tokens for tool definitions
    /// - attachments: set to 0 (caller should add attachment tokens)
    ///
    /// Uses non-blocking try_read to avoid UI freezes during LLM streaming.
    pub async fn context_breakdown(&self) -> ContextBreakdown {
        // Try to get the lock without blocking - during streaming, the write lock
        // is held by chat_streaming, and we don't want to freeze the UI waiting
        match self.chat_agent.try_read() {
            Ok(agent) => {
                let system_prompt = agent.system_prompt_tokens();
                let conversation_history = agent.conversation_history_tokens();
                let tool_schemas = agent.tool_schema_tokens();
                let max_tokens = agent.max_context_tokens();

                // Attachments are managed at the service layer, not here
                ContextBreakdown::new(
                    system_prompt,
                    conversation_history,
                    tool_schemas,
                    0, // Attachments will be added by AppService
                    max_tokens,
                )
            }
            Err(_) => {
                // Lock is contended (streaming in progress) - return empty breakdown
                // The UI will update properly once streaming completes
                ContextBreakdown::default()
            }
        }
    }

    /// Compact context (remove older messages)
    ///
    /// Uses LLM to generate a summary of older messages, then replaces them
    /// with the summary to free up context space.
    pub async fn compact_context(&self) -> Result<CompactionResult, ConversationError> {
        let mut agent = self.chat_agent.write().await;

        match agent.compact().await {
            Ok(Some(result)) => {
                let messages_removed = result.old_messages.saturating_sub(result.new_messages);
                Ok(CompactionResult {
                    old_tokens: result.old_tokens,
                    new_tokens: result.new_tokens,
                    messages_removed,
                    summary: Some(result.summary),
                })
            }
            Ok(None) => {
                // Not enough messages to compact
                Err(ConversationError::Other(anyhow::anyhow!(
                    "Not enough messages to compact (need at least 4)"
                )))
            }
            Err(e) => Err(ConversationError::Other(e)),
        }
    }

    /// Check if context should be compacted (80% threshold)
    ///
    /// Uses non-blocking try_read - if lock is contended, returns false
    /// (compaction check will happen after streaming completes)
    pub async fn should_compact(&self) -> bool {
        match self.chat_agent.try_read() {
            Ok(agent) => agent.is_context_near_limit(),
            Err(_) => false, // Don't compact while streaming
        }
    }

    /// Restore conversation from a session
    pub async fn restore_from_session(&self, session: &ChatSession) {
        let mut conv = self.conversation_mgr.write().await;
        conv.restore_from_session(session);

        // Sync session ID to ToolRegistry for pattern tracking
        let mut agent = self.chat_agent.write().await;
        agent.set_session_id(session.id.clone());
    }

    /// Set the session ID for tool pattern tracking
    pub async fn set_session_id(&self, session_id: String) {
        let mut agent = self.chat_agent.write().await;
        agent.set_session_id(session_id);
    }

    /// Clear conversation history
    pub async fn clear_history(&self) {
        let mut conv = self.conversation_mgr.write().await;
        conv.clear_messages();
        conv.clear_streaming();

        // Also clear the ChatAgent's conversation context
        let mut agent = self.chat_agent.write().await;
        agent.clear_history();
    }

    /// Reset context breakdown (called after clearing conversation and context)
    pub async fn reset_context_breakdown(&self) {
        // The context breakdown is computed from the ChatAgent's messages
        // After clear_history, it should already be reset
        // This method exists for explicit reset when clearing context files too
        let mut agent = self.chat_agent.write().await;
        agent.clear_history();
    }

    /// Get message count
    pub async fn message_count(&self) -> usize {
        let conv = self.conversation_mgr.read().await;
        conv.message_count()
    }

    /// List session approval patterns (for policy modal)
    pub fn list_session_patterns(
        &self,
        session_id: &str,
    ) -> anyhow::Result<(
        Vec<crate::policy::ApprovalPatternEntry>,
        Vec<crate::policy::ApprovalPatternEntry>,
    )> {
        // Try to get a read lock on the chat agent
        if let Ok(agent) = self.chat_agent.try_read() {
            agent.list_session_patterns(session_id)
        } else {
            // If we can't get the lock, return empty lists
            Ok((Vec::new(), Vec::new()))
        }
    }
}

#[cfg(test)]
mod tests {
    use super::ConversationService;
    use crate::agent::ChatAgent;
    use crate::llm::{LlmProvider, LlmResponse, Message, ToolDefinition};
    use async_trait::async_trait;
    use std::sync::atomic::Ordering;
    use std::sync::Arc;
    use tokio::sync::mpsc;

    struct TestProvider {
        name: &'static str,
    }

    #[async_trait]
    impl LlmProvider for TestProvider {
        fn name(&self) -> &str {
            self.name
        }

        async fn chat(
            &self,
            _messages: &[Message],
            _tools: Option<&[ToolDefinition]>,
        ) -> anyhow::Result<LlmResponse> {
            Ok(LlmResponse::Text {
                text: self.name.to_string(),
                usage: None,
            })
        }

        async fn complete_fim(
            &self,
            _prefix: &str,
            _suffix: &str,
            _language: &str,
        ) -> anyhow::Result<crate::llm::CompletionResult> {
            unimplemented!("test provider")
        }

        async fn explain_code(&self, _code: &str, _context: &str) -> anyhow::Result<String> {
            unimplemented!("test provider")
        }

        async fn suggest_refactorings(
            &self,
            _code: &str,
            _context: &str,
        ) -> anyhow::Result<Vec<crate::llm::RefactoringSuggestion>> {
            unimplemented!("test provider")
        }

        async fn review_code(
            &self,
            _code: &str,
            _language: &str,
        ) -> anyhow::Result<Vec<crate::llm::CodeIssue>> {
            unimplemented!("test provider")
        }
    }

    #[tokio::test]
    async fn update_llm_provider_rejects_while_processing() {
        let (tx, _rx) = mpsc::unbounded_channel();
        let tools = crate::tools::ToolRegistry::new(std::path::PathBuf::from("."));
        let agent = ChatAgent::new(Arc::new(TestProvider { name: "a" }), tools);
        let service = ConversationService::new(agent, tx);

        service.is_processing.store(true, Ordering::SeqCst);

        let result = service
            .update_llm_provider(Arc::new(TestProvider { name: "b" }))
            .await;

        assert!(result.is_err());
    }

    #[tokio::test]
    async fn update_llm_provider_allows_when_idle() {
        let (tx, _rx) = mpsc::unbounded_channel();
        let tools = crate::tools::ToolRegistry::new(std::path::PathBuf::from("."));
        let agent = ChatAgent::new(Arc::new(TestProvider { name: "a" }), tools);
        let service = ConversationService::new(agent, tx);

        let result = service
            .update_llm_provider(Arc::new(TestProvider { name: "b" }))
            .await;

        assert!(result.is_ok());
    }
}
