# Tark Configuration Example
# Save this to: ~/.config/tark/config.toml

[llm]
# Default provider to use
default_provider = "tark_sim"

# Enabled providers to show in TUI provider picker
# Empty list = show all providers from models.dev
# Specify only the providers you want to use
enabled_providers = ["tark_sim", "openai", "google", "claude", "ollama"]

[llm.tark_sim]
model = "tark_llm"
max_tokens = 8192

[llm.openai]
model = "gpt-4o"
max_tokens = 16384

[llm.claude]
model = "claude-sonnet-4-20250514"
max_tokens = 16384

[llm.gemini]
model = "gemini-2.0-flash-exp"
max_tokens = 8192

[llm.openrouter]
model = "anthropic/claude-3.5-sonnet"
max_tokens = 8192

[llm.ollama]
base_url = "http://localhost:11434"
model = "codellama"

[llm.copilot]
model = "gpt-4o"
max_tokens = 4096

[server]
port = 8765
host = "127.0.0.1"

[tui]
theme = "catppuccin_mocha"
plugin_widget_poll_ms = 2000
session_usage_poll_ms = 1000

[completion]
cache_size = 100
debounce_ms = 150
context_lines_before = 50
context_lines_after = 10

[agent]
# Maximum number of agent steps (tool calls) per message
# Increase for complex multi-step tasks
max_iterations = 50
timeout_seconds = 300

[tools]
shell_enabled = true

[thinking]
# Default thinking level: "off", "brief", "normal", "extended"
default = "off"
# Allow user to override thinking level
allow_override = true
# Maximum thinking tokens (for extended thinking)
max_tokens = 10000
